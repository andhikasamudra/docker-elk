input {
	beats {
		port => 5044
	}

	tcp {
		port => 50000
	}
	stdin {}
}

filter {
	# Ensure JSON is correctly parsed
	json {
		source => "message"
	}

	# Create a unique identifier based on the log message
	fingerprint {
		source => "message"
		target => "[@metadata][fingerprint]"
		method => "SHA256"
	}

	# Aggregate logs to deduplicate error messages and ensure unique fields
	aggregate {
		task_id => "%{[@metadata][fingerprint]}"
		code => "
		map['message'] ||= event.get('message')
		map['level'] ||= event.get('level')
		map['caller'] ||= event.get('caller')
		map['operation'] ||= event.get('operation')
		map['operation_time_ms'] ||= event.get('operation_time_ms')
		map['stacktrace'] ||= event.get('stacktrace')
		if event.get('error')
			map['error'] ||= event.get('error')
		end
		"
		push_map_as_event_on_timeout => true
		timeout => 5
		timeout_code => "
		event.set('message', map['message'])
		event.set('level', map['level'])
		event.set('caller', map['caller'])
		event.set('operation', map['operation'])
		event.set('operation_time_ms', map['operation_time_ms'])
		event.set('stacktrace', map['stacktrace'])
		if map['error']
			event.set('error', map['error'])
		end
		"
	}

	mutate {
		rename => {
			"@timestamp" => "log_timestamp"
		}
		add_field => {
			"timestamp" => "%{log_timestamp}"
		}
		convert => {
			"operation_time_ms" => "integer"
		}
		# Update fields only if they are missing
		update => {
			"level" => "%{[level]}"
			"caller" => "%{[caller]}"
			"operation" => "%{[operation]}"
			"operation_time_ms" => "%{[operation_time_ms]}"
			"stacktrace" => "%{[stacktrace]}"
		}
		remove_field => ["log_timestamp"]
	}
}

output {
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "logstash_internal"
		password => "${LOGSTASH_INTERNAL_PASSWORD}"
	}
}